{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ai_ngờ  akem  alo   an  angry_face   anh_chị  anxious_face_with_sweat  \\\n",
      "0       0.0   0.0  0.0  0.0    0.000000  0.000000                 0.000000   \n",
      "1       0.0   0.0  0.0  0.0    0.000000  0.000000                 0.000000   \n",
      "2       0.0   0.0  0.0  0.0    0.000000  0.222757                 0.000000   \n",
      "3       0.0   0.0  0.0  0.0    0.000000  0.000000                 0.000000   \n",
      "4       0.0   0.0  0.0  0.0    0.000000  0.000000                 0.000000   \n",
      "..      ...   ...  ...  ...         ...       ...                      ...   \n",
      "270     0.0   0.0  0.0  0.0    0.000000  0.000000                 0.000000   \n",
      "271     0.0   0.0  0.0  0.0    0.000000  0.000000                 0.000000   \n",
      "272     0.0   0.0  0.0  0.0    0.000000  0.000000                 0.000000   \n",
      "273     0.0   0.0  0.0  0.0    0.000000  0.000000                 0.000000   \n",
      "274     0.0   0.0  0.0  0.0    0.214063  0.000000                 0.107031   \n",
      "\n",
      "     ban_công  ban_đầu      bao  ...       ảnh  ảnh_hưởng   ấm_cúng  ấn_tượng  \\\n",
      "0         0.0      0.0  0.00000  ...  0.000000        0.0  0.000000       0.0   \n",
      "1         0.0      0.0  0.00000  ...  0.000000        0.0  0.000000       0.0   \n",
      "2         0.0      0.0  0.00000  ...  0.000000        0.0  0.222757       0.0   \n",
      "3         0.0      0.0  0.00000  ...  0.000000        0.0  0.000000       0.0   \n",
      "4         0.0      0.0  0.00000  ...  0.000000        0.0  0.000000       0.0   \n",
      "..        ...      ...      ...  ...       ...        ...       ...       ...   \n",
      "270       0.0      0.0  0.00000  ...  0.000000        0.0  0.000000       0.0   \n",
      "271       0.0      0.0  0.00000  ...  0.000000        0.0  0.000000       0.0   \n",
      "272       0.0      0.0  0.00000  ...  0.136076        0.0  0.000000       0.0   \n",
      "273       0.0      0.0  0.00000  ...  0.000000        0.0  0.000000       0.0   \n",
      "274       0.0      0.0  0.09971  ...  0.000000        0.0  0.000000       0.0   \n",
      "\n",
      "     ếch   ọ_   ồn   ổn  ổntom   ớt  \n",
      "0    0.0  0.0  0.0  0.0    0.0  0.0  \n",
      "1    0.0  0.0  0.0  0.0    0.0  0.0  \n",
      "2    0.0  0.0  0.0  0.0    0.0  0.0  \n",
      "3    0.0  0.0  0.0  0.0    0.0  0.0  \n",
      "4    0.0  0.0  0.0  0.0    0.0  0.0  \n",
      "..   ...  ...  ...  ...    ...  ...  \n",
      "270  0.0  0.0  0.0  0.0    0.0  0.0  \n",
      "271  0.0  0.0  0.0  0.0    0.0  0.0  \n",
      "272  0.0  0.0  0.0  0.0    0.0  0.0  \n",
      "273  0.0  0.0  0.0  0.0    0.0  0.0  \n",
      "274  0.0  0.0  0.0  0.0    0.0  0.0  \n",
      "\n",
      "[275 rows x 1312 columns]\n"
     ]
    }
   ],
   "source": [
    "#Reference source: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/\n",
    "\n",
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "\n",
    "# assign documents\n",
    "v=[]\n",
    "textt=''\n",
    "f = open(\"D:/Tài liệu học/kì 5 năm 3/Khai phá dữ liệu web/kpw/4_12_Chapter4Final/4_12_Chapter4Final/stopwordvn.txt\", \"r\", encoding=\"utf-8\")\n",
    "#Get Stop words Dictionaries\n",
    "List_StopWords=f.read().split(\"\\n\")\n",
    "#Read file text\n",
    "src='D:/Tài liệu học/kì 5 năm 3/Khai phá dữ liệu web/kpw/4_12_Chapter4Final/Data4'\n",
    "path=os.listdir(src)\n",
    "\n",
    "for i in path:\n",
    "    label=i.split(\"'\")[0]\n",
    " \n",
    "    f=open('D:/Tài liệu học/kì 5 năm 3/Khai phá dữ liệu web/kpw/4_12_Chapter4Final/Data4/'+str(label), \"r\", encoding=\"utf-8\")\n",
    "    text=f.read()\n",
    "#################################\n",
    "######## Text Processing ########\n",
    "#################################\n",
    "    text_pre=text.replace(\"\\n\",\"\")  # Remove the newline command\n",
    "    text_pre=text.lower() # Convert text to lowercase\n",
    "    text_pre = re.sub(\"\\d+\", \" \", text_pre) # Remove number\n",
    "    \n",
    "    text_pre = re.sub(r\"[!@#$[]()]'\", \"\", text_pre) # Remove character: !@#$[]()\n",
    "    text_pre=emoji.demojize(text_pre)\n",
    "    stop = stopwords.words('english')   # Remove StopWords\n",
    "    text_pre = \" \".join(text_pre for text_pre in text_pre.split() if text_pre not in List_StopWords)\n",
    "    textt=textt +text_pre \n",
    "\n",
    "\n",
    "v=textt.split(\" . \")\n",
    "Data=[]\n",
    "for i in v:\n",
    "    text_i=re.sub(r'[^\\w\\s]','',i) # Remove punctuation\n",
    "    Data.append(text_i)\n",
    "\n",
    "\n",
    "tr_idf_model  = TfidfVectorizer()\n",
    "tf_idf_vector = tr_idf_model.fit_transform(Data)\n",
    "\n",
    "tf_idf_array = tf_idf_vector.toarray()\n",
    "# print(tf_idf_array)\n",
    "\n",
    "W = tr_idf_model.get_feature_names_out()\n",
    "# print(W)\n",
    "\n",
    "df_tf_idf = pd.DataFrame(tf_idf_array, columns = W)\n",
    "print(df_tf_idf)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6420907d571700bda39d91c56e2e34986da203e291e814ba6901cd8edae2741b"
  },
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
